# Preface

前言部分主要讲了下该教材出第二版的一些原因，其主要要点如下：

1. 第一版出版于1998年，在出版后的二十年间人工智能取得了巨大的进步，这股浪潮由更强大的计算能力和新理论、新算法共同推动。
2. 书中未给出强化学习的严格形式化定义，也没有将其推广到最一般的形式。整本书依然是导论性质的书，重点是Online Learning Algorithms。
3. 第二版相比于第一版，需要更高程度的数学解释，书中放在灰色框中的数学内容对于那些非数学背景的读者而言是可以跳过的。
4. 第二版对符号进行了优化，方便人们学习理解。Random Variables（随机变量）用大写字母表示，比如St, At, Rt代表时间步t的状态、动作和奖励；而其具体取值则用小写字母表示，比如上述变量对应的具体的值可能是s, a, r.
5. 值函数用小写字母表示，如v pi；表格形估计用大写字母表示，如Qt(s, a)。近似值函数是确定性函数，因此参数也采用小写字母；大写粗体字母则用于矩阵。
6. 第二版减少了上标和下标的过度使用，采用更加简便的p(s', r | s, a)这样的写法。
7. 本书内容推荐用两个学期来学习完成；如果只学习一个学期的话，推荐覆盖前十章。（读者注：难怪我上RL课的时候感觉基本讲到第十章左右就进入期末项目了，原来出处是这里）
8. Sutton（在本系列笔记中Sutton就代表Sutton和Barto，后面不再赘述）感谢了科学家A. Harry Klopf，并表示第二版献给他。Klopf教授提出了脑与人工智能的构想，让两位作者相识；在Klopf的引荐下，还在斯坦福读本科的Sutton加入了新创办的UMass Amherst项目的研究小组。Klopf是让项目获得资助的关键人物。
9. Sutton感谢了阿尔伯塔大学的强化学习与人工智能实验室。
10. Klopf是早期关注机器智能领域的重要人物，提出了一些对人工智能产生重大影响的理论。他认为最大化某种内部目标，无论是具体的还是抽象的，是智能系统的核心，而非单纯的维持系统稳定。换句话说，智能体的目标不仅是保持平衡，还应该能够主动探索环境，根据经验预测未来，并选择行动以最大化其长期回报或内部奖励。这种哲学观点直接启发了后来的强化学习理论：智能体不仅响应环境变化，也通过主动选择行为来优化长期目标；智能体不是模仿固定规则或者维持状态，而是通过试错学习最大化累积奖励。Sutton和Barto直言这种思想正是他们强化学习研究旅程的思想起点。

然后是第一版的前言，该部分主要讲述了强化学习研究的由来：

1. 1979年末，两位作者在UMass工作的时候参与了一个早期项目，该项目主要内容是去尝试唤起这样一种观点：由类神经网络的自适应单元组成的网络可能为实现人工自适应智能提供一种有前途的途径。该项目研究了Klopf提出的异态系统理论（Heterostatic Theory of Adaptive Systems），并进行了批判性的探讨。
2. 在之后的二十年里，Sutton一直都没有结束对该思想的研究；这个简单的思想长期以来被视为是理所当然的，却从未在计算角度受到重视，该思想的主要内容是：一个学习系统想要某样东西，他会调整自己的行为，来最大化从环境中获得的某个特殊信号。这就是所谓的Hedonistic（享乐主义）学习系统思想，也就是如今我们所说的强化学习系统。
3. 一开始的时候，Sutton和Barto以为强化学习已经被充分探讨了，结果深入研究后发现根本就没怎么被研究过。大多数研究者只是浅浅地研究了强化学习，然后后来基本都转向了其他方向，比如Pattern Classification（模式分类）、Supervised Learning（监督学习）和Adaptive Control（自适应控制）；或者他们干脆完全放弃了对学习问题地研究。因此，在两位作者刚开始研究的时候，发现如何从环境中获取有益东西的学习问题根本就没有受到足够的关注，这个情况直接导致强化学习这个分支尚未被充分探索，在计算角度也无真正进展。
4. 作者认为，虽然截至第一版时“如何通过交互式学习实现目标”这一问题远未被彻底解决，但是他们对他的理解已经大为加深。TD时序差分学习、动态规划、函数逼近等已经可以整合进一个整体一致地视角中了。
5. 写这本书的目的是提供一本清晰而简明地教材，系统性介绍强化学习地关键思想与算法。本书采用了人工智能和工程的视角进行讲解。
6. 在第一版中也提到了，本书没有对强化学习进行严格的形式化处理，因为作者不追求最高层次的数学抽象，也不使用严格的定理-证明格式。在简洁性与理论普适性之间保持一定的平衡来兼顾不分散读者注意力和帮助数学更强的读者走向更深入的理解。
7. 1998年第一版中，作者便说，为这本书他们已经努力了三十年。（读者注：这种探索科学的精神真的令人感动）
8. 读者注：作者在1998年第一版的感谢的名单中，有一位叫做Wei Zhang的中国人，没有查到该人是谁。如今我们在学习已经火遍全球的强化学习的时候，很难想象在三十到五十年前，已经有很多先驱进行了充分的探索了。有时候我想在阿尔伯塔这样偏僻而远离喧嚣的地方能坐落着一个世界顶级的人工智能研究所，属实令人称奇。这些伟大的科学成就的起点竟然都是如此地“不起眼”，截至1998年Sutton的论文被引用次数也不过一千多次，但是他们却已经付出了三十年的深刻的思考与研究。前两天我看了几个Sutton的采访视频，他依然在深度思考人工智能和强化学习的未来。相比于某些功成名就后的炫技者，他更像一个哲学家：永远在思考“智能究竟是什么”这一根本问题。

除了上述前言外，我还想分享一下Sutton在2019年写的一篇叫做《The Bitter Lesson》的文章，该文章并不长，但总结了他对于人工智能发展七十年的一些重要思考和看法，我收录了其中一些比较重要的点：

1. 人工智能研究七十年来最大的教训就是：基于通用方法、依赖计算力的方法，最终是最有效的——而且是大幅度更有效。造成这一点的根本原因是摩尔定律，或者更广义地说，是每单位计算成本呈指数下降的长期趋势。然而，大多数人工智能研究都是在一个隐含前提下进行的：即，智能体可以使用的计算资源是恒定的。在这种假设下，融入人类知识似乎是提高性能的唯一途径。但现实是：只要时间略微延长，远超当前计算资源的能力就将出现。因此，虽然研究者们为了在短期内有所突破而倾向于使用自己对领域的知识，但从长远看，真正重要的事情只有一个：能否有效利用不断增长的计算能力。
2. 靠人类知识和靠计算扩展在理论上并非必然对立，但是在实践中却往往如此。一方面，研究者的时间只能投入其一；另一方面，人们会在某种方法上形成心理依附。同时，人类知识路径往往导致算法变得更复杂，从而变得不适合与大规模通用计算方法相结合。
3. 1997年击败世界冠军卡斯帕罗夫的，是一种基于大规模深度搜索的方法。当时的主流计算机棋类研究者们普遍感到沮丧，因为他们长期以来都在努力构建基于人类对棋局理解的系统，但最终胜出的却是一个相对“简单”、依赖特殊硬件和软件优化的搜索型系统。很多研究者们希望融合人类智慧的方法能赢，但是当现实相反时，他们对暴力搜索的大获全胜感到失望。
4. 围棋中也出现了类似的事情。最初研究者们投入大量努力去避开搜索，转而利用人类知识或游戏本身的特殊性质，但这些努力最终都被证明是无关紧要甚至适得其反的。可以说，搜索和学习是现代人工智能中最重要的两大计算扩展技术。就像在国际象棋中一样，围棋研究起初也试图借助人类理解来减少搜索的需求，但直到最终才意识到真正的成功来自于接受并彻底利用搜索与学习。
5. 1970年代，美国国防部组织过一次语音识别竞赛，最终使用更加统计化、计算化的模型战胜了依赖大量人类知识的方法。统计方法的胜出引发了整个自然语言处理领域的长期转变，在接下来的几十年里，统计与计算逐渐成为主导力量。而近年来的深度学习更进一步推动了语音识别的进步，而该方法更加依赖计算，几乎不依赖人类知识。
6. 研究人员常常试图让系统像他们自己一样思考，将自己的知识硬编码进系统，这往往在最后都被证明是有害的。因为随着计算能力的指数级增长，那些依赖人类输入的系统显得低效而过时时，反而浪费了研究者的大量时间。
7. 在计算机视觉领域，现代深度神经网络替代了人类手工设计的抽象表示。卷积和不变性的基本结构的性能远超早期模型。
8. 我们如今仍在重复类似的错误。要识别并对抗这些错误，我们首先要理解他们的诱惑：在系统中构建我们以为“自己是如何思考的方式”看似自然，却在长期上是失败的捷径。（读者注：最近的采访中有人问他如何看待RLHF，他表示不看好，其具体原因没细说，但可以参考这里的这段话。）
9. 痛苦的教训来自于历史观察，总结下来就是：AI研究者总是试图将知识硬编码到系统中，这在短期内确实有效，而且能给研究者带来心理满足；但是长期来看，这种方法会遇到瓶颈，甚至阻碍进步。真正突破性的进展最终总是来自于相反的方向：依赖搜索与学习，扩大计算规模，而非增加知识注入。
10. 这种成功之所以“苦涩”，是因为他击败了人类倾注热情与信念的人本主义路径。
11. 我们应该学到的第一点是：具有普适性的通用方法（可随计算力扩展）是强大的根本。
12. 另一个教训则是：别再幻想“简单地理解大脑”。心智内容本身是极其、不可简化的复杂。我们不应再试图寻找简单的方法来描述心智的内容，比如空间是什么、对象是什么、对称性如何处理等——这些都是外部世界本身的任意复杂性的一部分。我们不应试图将这些复杂性预设为只是内建，而应该构建那些能够自己发现这些复杂性的元方法（Meta Methods）。这些方法的关键在于他们能找到良好的近似，但找到这些近似的过程应由系统自己完成，而不是由人类来提前构造。
13. 我们想要的是：能像我们一样发现的智能体，而不是包含了我们已经“发现”过的知识的系统。因为，把我们发现的东西硬塞进去，反而会妨碍我们理解“发现”的过程是如何可能的。

读者注：前言部分让我大受震撼，可以说作者们把自己几十年的研究思考浓缩在这里了。本来只是想着这个夏天时间很多所以想认真精读一下这本经典教材，没想到上来就是一个大受震撼。最近我自己在开发一个注入了很多财务知识的AI Agent，看完后意识到这个方法并不可行。其实在开发的初期我就发现了很多不美观的地方，所以在机器学习课程最终的期末项目中，我新建了一个纯LLM的layer，然后在下一层才使用我自己构建的工具或者LangChain、OpenManus这种工具系统。我在项目报告中的反思章节提出了我对AI Agent的看法：我认为AI Agent在最终根本不需要任何人为构建的工具链或者PROMPT，所有的任务都可以直接通过LLM自身对任务的理解来完成，我称之为“Tool-Free AI Agent”。如果LLM拥有足够的上下文理解能力、编程能力和记忆能力，那么就不需要复杂的工具链，只需要让LLM自己来写代码然后动态调用接口就能够完成任务了。今天整理这篇文章的时候看了Sutton的The Bitter Lesson，确信了我之前的思考是正确的。AI Agent的未来一定也和之前所有的惨痛教训一样，RLHF、PROMPT ENGINEERING等最终都会被扫进历史的垃圾堆里。我很庆幸自己这次回到学校学习计算机科学，现在每天的学习都处在一种纯享与思考的过程中，不为找工作，不为考试，不为升学。这或许是作为一个成长于中国的应试教育环境，在工作几年后回到学校追求学习本身的纯粹所带来的极致快乐吧。
